{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Mathematics and Programming in Artificail intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Why CIFAR-10](#why-cifar-10)\n",
    "2. Task 1: NumPy Neural Network Implementation\n",
    "   - [Activation Functions](#Activation-Functions)\n",
    "   - [Softmax Layer](#Softmax-Layer)\n",
    "   - [Dropout Implementation](#Dropout-Implementation)\n",
    "   - [Neural Network Class](#neural-network-class)\n",
    "   - [Optimisers](#optimisers)\n",
    "   - [Network Evaluation and Results](#network-evaluation-and-results)\n",
    "3. Task 2: PyTorch Implementation\n",
    "   - [Dataset Preparation](#dataset-preparation)\n",
    "   - [Evaluations which will be used to measure performance of each CNN variant](#evaluations-which-will-be-used-to-measure-performance-of-each-cnn-variant)\n",
    "   - [Model Description and Implementation](#model-description-and-implementation)\n",
    "   - [Improved model](#improved-model)\n",
    "   - [optimised hyperparameters](#optimised-hyperparameters)\n",
    "   - [Results and Discussion](#results-and-discussion)\n",
    "4. [Conclusion and Reflection](#conclusion-and-reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose CIFAR-10 dataset due to its complexity and suitability in evaluating multi-layer neural networks. It holds 60,000 32x32 colour images spanning 10 diverse classes, crescendoing in a challenging classification task surpassing the likes of datasets like MNIST, which only holds grayscale digits. CIFAR-10 includes RGB images, demanding models to learn from more detailed and complex data, mirroring real world applications, where data is diverse and high dimensional. The aforementioned complexity allows rigorous testing of network architectures, activation functions, and techniques to optimise the model. Lastly, it has well documented benchmarks, and  widespread use in academic research making it a top candidate to showcase advanced implementations, setting the ground work for meaningful comparisons and evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: NumPy Neural Network Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid functions and ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    @staticmethod\n",
    "    def sigmoidForward(x):\n",
    "        '''\n",
    "        this function does the forward pass of the sigmoid function\n",
    "        it takes an input array i just called x \n",
    "        and it returns a tuple with the result (out) of the sigmoid function\n",
    "        as well as cache which we use in the backward pass, its just the same as the out \n",
    "        '''\n",
    "        out = 1/ (1+ np.exp(-x)) #the sigmoid function\n",
    "        cache = out \n",
    "        return out, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoidBackward(dout, cache):\n",
    "        '''\n",
    "        does the backward pass of the simoid function\n",
    "        i used d to show that its the derivative \n",
    "        so dx is the gradient of the loss with resepct to x (the input array)\n",
    "        dout is the upstream gradient\n",
    "        sig is just the sigmoid function hence why it equals cache\n",
    "        '''\n",
    "        sig = cache\n",
    "        dx = dout * sig * (1 - sig) #the derivative of the sigmoid function multiplied by the upstream gradient to get the proper flow of gradients\n",
    "        return dx\n",
    "    \n",
    "    @staticmethod   \n",
    "    def reluForward(x):\n",
    "        '''\n",
    "        does the forward pass of the ReLU function\n",
    "        x is the input arry \n",
    "        and then it outputs a tuple with the result of the  ReLU forward pass as well a cache used for backward pass\n",
    "        cache this time is the input array \n",
    "        '''\n",
    "        out = np.maximum(0,x)\n",
    "        cache = x \n",
    "        return out, cache\n",
    "    @staticmethod\n",
    "    def reluBackward(dout, cache):\n",
    "        '''\n",
    "        backward pass of the ReLU function\n",
    "        x is just passing on the inpui arrat from forward pass using cache as the temporary store \n",
    "        dx is the gradient of the loss in respect to the input (being the array x)\n",
    "        dout is the upstream gradient \n",
    "        '''\n",
    "        x = cache \n",
    "        dx = dout * (x > 0) #derivative is 1 when x >0 otherwise it is 0\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        # Prepare to store the output of the softmax function\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, logits):\n",
    "        \"\"\"\n",
    "        Perform the forward pass to calculate softmax probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        logits (np.array): Scores from the previous layer, shaped (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "        np.array: Probabilities for each class, same shape as input.\n",
    "        \"\"\"\n",
    "        # Subtract the max to keep numbers stable\n",
    "        z_max = np.max(logits, axis=1, keepdims=True)\n",
    "        shifted_logits = logits - z_max\n",
    "        exp_shifted = np.exp(shifted_logits)\n",
    "\n",
    "        # Divide by sum of exponents to get probabilities\n",
    "        sum_exp = np.sum(exp_shifted, axis=1, keepdims=True)\n",
    "        self.output = exp_shifted / sum_exp\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, true_labels):\n",
    "        \"\"\"\n",
    "        Perform the backward pass to calculate gradient of the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        true_labels (np.array): One-hot encoded true class labels.\n",
    "\n",
    "        Returns:\n",
    "        np.array: Gradient of the loss with respect to logits.\n",
    "        \"\"\"\n",
    "        # Get the number of samples to average the gradient\n",
    "        num_samples = true_labels.shape[0]\n",
    "\n",
    "        # Calculate the gradient for softmax combined with cross-entropy\n",
    "        gradient = (self.output - true_labels) / num_samples\n",
    "        return gradient\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage of SoftmaxLayer\n",
    "\n",
    "    # Define example logits, representing class scores\n",
    "    logits_example = np.array([[2.0, 1.0, 0.1],\n",
    "                               [1.0, 2.0, 0.1]])\n",
    "\n",
    "    # Define the true labels in one-hot encoding\n",
    "    true_labels_example = np.array([[1, 0, 0],\n",
    "                                    [0, 1, 0]])\n",
    "\n",
    "    # Instantiate the softmax layer\n",
    "    softmax_layer = SoftmaxLayer()\n",
    "\n",
    "    # Forward pass to get probability distributions\n",
    "    softmax_output = softmax_layer.forward(logits_example)\n",
    "    print(\"Softmax Probabilities:\\n\", softmax_output)\n",
    "\n",
    "    # Backward pass to compute gradient\n",
    "    loss_gradient = softmax_layer.backward(true_labels_example)\n",
    "    print(\"Gradient of Loss w.r.t Logits:\\n\", loss_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Author: Abdelrahmane Bekhli\n",
    "    Date: 2024-11-18\n",
    "    Description: This class performs dropouts.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropoutRate, seed=None):\n",
    "        \"\"\" \n",
    "        Initialize the Dropout layer. \n",
    "        Args: \n",
    "            dropoutRate (float): The probability of dropping out a unit. \n",
    "            seed (int, optional): Random seed for reproducibility. \n",
    "        \"\"\"\n",
    "        if not (0 <= dropoutRate < 1):\n",
    "            raise ValueError(\"Dropout rate must be between 0 and 1\")\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for dropout.\n",
    "        Args:\n",
    "            x (numpy array): The input to the dropout layer.\n",
    "        Returns:\n",
    "            numpy array: Output after applying dropout.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropoutRate\n",
    "            return x * self.mask / (1 - self.dropoutRate)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass for dropout.\n",
    "        Args:\n",
    "            dout (numpy array): The gradient from the next layer.\n",
    "        Returns:\n",
    "            numpy array: Gradient after applying dropout mask.\n",
    "        \"\"\"\n",
    "        return dout * self.mask / (1 - self.dropoutRate)\n",
    "    \n",
    "    def setMode(self, mode):\n",
    "        \"\"\"\n",
    "        Set the mode for the network: 'train' or 'test'\n",
    "        Args:\n",
    "            mode (str): Either 'train' or 'test'.\n",
    "        \"\"\"\n",
    "        if mode == 'train':\n",
    "            self.training = True\n",
    "        elif mode == 'test':\n",
    "            self.training = False\n",
    "        else:\n",
    "            raise ValueError(\"Mode can only be 'train' or 'test'\")\n",
    "        self.mask = None # reset mask when changing modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Check if a GPU is open, if it is use CUDA for faster computation, if not go with CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # Randomly crop a patch that is 32x32 with padding, improving robustness to spatial shifts\n",
    "    transforms.RandomHorizontalFlip(),  # Flip the image horizontally to augment data \n",
    "    transforms.ToTensor(),  # Convert from PIL to PyTorch tensors format\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Add color jitter to simulate varied lighting conditions\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images to have a mean of 0.5 and std of 0.5 for stability\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Only normalize test data; augmentation not needed during evaluation\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with the specified transformations\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "# Concatenate the training and test datasets\n",
    "combined_data = ConcatDataset([train_data, test_data])\n",
    "\n",
    "# train/validation split ratio\n",
    "train_size = int(0.72 * len(combined_data))  # 72% for training\n",
    "validation_size = int(0.18 * len(combined_data))  # 18% for validation\n",
    "test_size = int(0.10 * len(combined_data))  # 10% for testing \n",
    "\n",
    "# Split the combined data into train, validate and test sets, in a randomized manner but with a fixed seed to ensure reproducibility of the results\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_data, validation_data, test_data = random_split(combined_data, [train_size, validation_size, test_size], generator)\n",
    "\n",
    "# Create data loaders for train, validation, and test sets\n",
    "load_train = DataLoader(train_data, batch_size=64, shuffle=True)  # Training data is in random order to help training\n",
    "load_validation = DataLoader(validation_data, batch_size=64, shuffle=False)  # No shuffling for validation data\n",
    "load_test = DataLoader(test_data, batch_size=64, shuffle=False)  # No shuffling for test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations which will be used to measure performance of each CNN variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made a class with all the evaluations i will use for my models to compare and illustrate their performance \n",
    "class ModelEvaluator:\n",
    "    #initialise with the attributes that we will use\n",
    "    def __init__(self, model, optimiser, criterion, device, load_train, load_validation, load_test):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.load_train = load_train\n",
    "        self.load_validation = load_validation\n",
    "        self.load_test = load_test\n",
    "        \n",
    "        #initialise empty list so that metrics can be tracked as training goes on\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.accuracies = []\n",
    "        self.all_labels = []\n",
    "        self.all_predictions = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def store_learning_rate(self):\n",
    "        lr = [group['lr'] for group in self.optimiser.param_groups] #gets the learning rate currently in use from the optimser\n",
    "        self.learning_rates.append(lr[0]) #adds that to a learning rates list to visualise wehn we draw grath\n",
    "\n",
    "    def evaluate_epoch(self, epoch):\n",
    "        self.model.eval() #puts model from training to evaluation mode, turning off droupt and batch updates\n",
    "        # init validation loss and counters for accuracy, also clear the previous lables and predictions for the epoch\n",
    "        validation_loss = 0.0 \n",
    "        correct, total = 0, 0\n",
    "        self.all_labels.clear()\n",
    "        self.all_predictions.clear()\n",
    "\n",
    "        #goes through the set without calculating gradients, computes: prediction, loss anad stores the actual labels as well as the predicted labels \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.load_validation:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                validation_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                self.all_labels.extend(labels.cpu().numpy())\n",
    "                self.all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        #compute the average validation loss as well as the accuracy for the epoch\n",
    "        average_validation_loss = validation_loss / len(self.load_validation)\n",
    "        accuracy = 100 * correct / total\n",
    "        #appends those results in the lists \n",
    "        self.validation_losses.append(average_validation_loss)\n",
    "        self.accuracies.append(accuracy)\n",
    "        print(f\"Epoch {epoch+1}: Validation Loss: {average_validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "#plotting accuracy plot\n",
    "    def plot_accuracy(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(self.accuracies)+1), self.accuracies, marker='o', linestyle='-', color='b')\n",
    "        plt.title(\"Model Accuracy Over Training\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "#validation loss plot \n",
    "    def plot_validation_loss(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(self.validation_losses)+1), self.validation_losses, marker='o', linestyle='-', color='r')\n",
    "        plt.title(\"Model Validation Loss Over Training\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "#training loss plot\n",
    "    def plot_training_loss(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(self.train_losses)+1), self.train_losses, marker='o', linestyle='-', color='g')\n",
    "        plt.title(\"Model Training Loss Over Training\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "#confusion matrix\n",
    "    def plot_confusion_matrix(self):\n",
    "        #makes a confusion matrix to show the perfomance of the classification\n",
    "        cm = confusion_matrix(self.all_labels, self.all_predictions)\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.7)\n",
    "        #inputts the numerical values to the matrix \n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center', color='black', fontsize=12)\n",
    "        #addinf the lables and titles so its easy to read \n",
    "        ax.set_xticks(range(len(self.load_validation.dataset.classes)))\n",
    "        ax.set_xticklabels(self.load_validation.dataset.classes, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(range(len(self.load_validation.dataset.classes)))\n",
    "        ax.set_yticklabels(self.load_validation.dataset.classes)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "#ouput a classsification report \n",
    "    def print_classification_report(self):\n",
    "        report = classification_report(self.all_labels, self.all_predictions, target_names=self.load_validation.dataset.classes)\n",
    "        print(\"\\nClassification Report:\\n\")\n",
    "        print(report)\n",
    "\n",
    "#plot a preciison recall curve \n",
    "    def plot_precision_recall_curve(self):\n",
    "        #computes the curve for each class\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i, class_name in enumerate(self.load_validation.dataset.classes):\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                np.array(self.all_labels) == i, np.array(self.all_predictions) == i\n",
    "            )\n",
    "            plt.plot(recall, precision, label=f'Class: {class_name}')\n",
    "        #displays it \n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "#learning rate plot\n",
    "    def plot_learning_rate(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(self.learning_rates)+1), self.learning_rates, marker='o', linestyle='-', color='m')\n",
    "        plt.title(\"Learning Rate Schedule\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "#figure out the top 5 accuracy, which is good for multi-class problems \n",
    "    def calculate_top5_accuracy(self):\n",
    "        correct_top5, total_top5 = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.load_validation:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, top5_predictions = outputs.topk(5, dim=1)\n",
    "                correct_top5 += (top5_predictions == labels.unsqueeze(1)).sum().item()\n",
    "                total_top5 += labels.size(0)\n",
    "        top5_accuracy = 100 * correct_top5 / total_top5\n",
    "        print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
    "\n",
    "#ROC CURVE \n",
    "    def plot_roc_curve(self):\n",
    "        #compute and plit teh ROC curve with the AUC score \n",
    "        labels_bin = label_binarize(self.all_labels, classes=range(len(self.load_validation.dataset.classes)))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i, class_name in enumerate(self.load_validation.dataset.classes):\n",
    "            fpr, tpr, _ = roc_curve(labels_bin[:, i], np.array(self.all_predictions) == i)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "We utilised a Convolutional Neural Network (CNN) model for this task as it is degined to efficiently learn hierarchical image features from low resolutions like 32 by 32 rgb images. The model is made up of three layers, each with increasing filter sizes be that 32,64 or 128 as well as 3 by 3 kernels, each followed by batch normalisation, ReLU activation, and max-pooling which reduces spatiali dimensions while maintaining critical features. \n",
    "After the aformentioned convolution stages, the network contains two fully connected layers, the first with 256 neurons and ReLU activation, which is then followed by dropout to prevent overfitting,\n",
    "the second includes 10 output neurorns representing classs probabilities for CIFAR-10's different categories.\n",
    "The model we have implemented leverages standard PyTorch components as well as cross-entropy to optimise accuracy in classification problems.\n",
    "\n",
    "### justification\n",
    "A CNN architecture is well suited for a dataset like CIFAR-10 due to its inate ability to learn complex spatial hierarchies by utilising layered convolutional operations, extreacting patterns such as textures, objects, and edges. \n",
    "The chosen model balances complexity with efficiency by making use of three convolutional layers, this allows computation to remain manageable while still capturing the most essential features.\n",
    "Batch normalisation enables stable and faster convergence, while making use of dropout prevents overfitting on the dataset which is relatively small.\n",
    "By combining max-pooling and fully connceted layers the model is able to have an optimal tradeoff between performance and training efficiency. In my opinion this all makes the Convolutional Neural Network an effective architecture for our chosen dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #convolutional layers \n",
    "        self.convolution1 = nn.Conv2d(3,32, kernel_size= 3, padding = 1) #conv2d applies 2d convolution\n",
    "        self.normaliseBatch1 = nn.BatchNorm2d(32) #normalise each batch activation to have training be stable and speed up convergence \n",
    "        self.convolution2 = nn.Conv2d(32,64, kernel_size= 3, padding =1)\n",
    "        self.normaliseBatch2 = nn.BatchNorm2d(64)\n",
    "        self.convolution3 = nn.Conv2d(64,128, kernel_size=3, padding =1)\n",
    "        self.normaliseBatch3 = nn.BatchNorm2d(128)\n",
    "    \n",
    "        self.pool = nn.MaxPool2d(2,2) #reduce dimensions, takes the max value in a 2 by 2 window, halves the width and heigth\n",
    "        # fully connected layer \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256) #input flattend output from last convolutional layer \n",
    "        self.fc2 = nn.Linear(256, 10) # output 256 in the first layer, 10 in the last \n",
    "        self.dropout = nn.Dropout(0.5) #randomly deactive half the neurons as to not oevrfit \n",
    "        self.relu = nn.ReLU() # apply activation fucntuion relu \n",
    "\n",
    "    def forward(self, x):\n",
    "        #the following three functions normalise activations, make it non-linear, reduce spatial dimension\n",
    "        x = self.pool(self.relu(self.normaliseBatch1(self.convolution1(x)))) \n",
    "        x = self.pool(self.relu(self.normaliseBatch2(self.convolution2(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch3(self.convolution3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)  # 2d features to 1d for fully connected layers\n",
    "        x = self.dropout(self.relu(self.fc1(x)))  #regularsing the flattened output after beiong passed through the first fully connected layer\n",
    "        x = self.fc2(x) #passes the output through the second fully connnected layer to get class score \n",
    "        return x #the raw prediction scores for each class in the dataaset\n",
    "\n",
    "\n",
    "model = CNN().to(device) #initialise and move to the device that we prepared in the above module\n",
    "criterion = nn.CrossEntropyLoss() #to get the difference between predictions and ground truth\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001) #adjusts the weights in the model based on the gradients\n",
    "\n",
    "# Create an instance of ModelEvaluator after training\n",
    "evaluator = ModelEvaluator(model, optimiser, criterion, device, load_train, load_validation, load_test)\n",
    "\n",
    "#loop for training\n",
    "epochs = 20 #training will iterate through 20 epochs over the whole dataset\n",
    "for epoch in range(epochs): #processed training for every epoch \n",
    "    model.train() #enables training and activates dropout\n",
    "    running_loss = 0.0  #initiate the loss counter which will be used to calc the average loss\n",
    "    for inputs, labels in load_train: #loops through the branches training data we had made in the data loader above\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # move both the image tensors from the current batch, and the corresponging true class lables to the selcted device\n",
    "        optimiser.zero_grad() #reset gradient from previous iteration of training \n",
    "        outputs = model(inputs) #forward pass, feeds input through the cnn, generating predictions for the batch\n",
    "        loss = criterion(outputs, labels) #calculating the loss function between predictions and labels using cross entropy loss \n",
    "        loss.backward() #back propagation gives us the gradient \n",
    "        optimiser.step() #update the parameters using the calculated gradient\n",
    "        running_loss += loss.item() #keeps track of the average loss per epoch \n",
    "        average_train_loss = running_loss / len(load_train)\n",
    "        evaluator.train_losses.append(average_train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(load_train):.4f}\") #output the current average loss for the epoch\n",
    "\n",
    " # Evaluate on Validation Set After Each Epoch\n",
    "    evaluator.evaluate_epoch(epoch)\n",
    "    evaluator.store_learning_rate()\n",
    "\n",
    "model.eval() #switch from training to evaluation\n",
    "\n",
    "# Evaluate model on the validation set after all epochs\n",
    "evaluator.evaluate_epoch(epochs - 1)\n",
    "\n",
    "#initialise the metrics\n",
    "correct = 0  #correct classifiers\n",
    "total = 0 #number of imaged evaluated\n",
    "with torch.no_grad(): #to save some memory prevent gradient computation while inference is running \n",
    "    for inputs, labels in load_validation: #go through test set\n",
    "        inputs, labels = inputs.to(device), labels.to(device) #move the images and corresponding lables to the device selected\n",
    "        outputs = model(inputs) #forward pass inputs into the cnn model, giving us a tensor with the raw prediction scores from the final layer\n",
    "        _, predicted = torch.max(outputs, 1) #choose the class with the higherst prediction score, return the predicted class index, the maximum vaye itseld is not used.\n",
    "        total += labels.size(0) #increase total by number of images in the batch to keep track of the ones processed\n",
    "        correct += (predicted == labels).sum().item() #compare prediction with labels\n",
    "\n",
    "final_validation_accuracy = 100 * correct / total\n",
    "print(f\"Final Validation Accuracy: {final_validation_accuracy:.2f}%\") #prints out the calculated accuracy % to 2 decimal places\n",
    "\n",
    "# Evaluate model on the test set\n",
    "evaluator.evaluate_epoch(epochs - 1)\n",
    "\n",
    "# Call all evaluation methods\n",
    "evaluator.plot_accuracy()               # Plot Accuracy\n",
    "evaluator.plot_validation_loss()        # Plot Validation Loss\n",
    "evaluator.plot_training_loss()          # Plot Training Loss\n",
    "evaluator.plot_confusion_matrix()       # Confusion Matrix\n",
    "evaluator.print_classification_report() # Classification Report\n",
    "evaluator.plot_precision_recall_curve()# Precision-Recall Curve\n",
    "evaluator.plot_learning_rate()          # Learning Rate Plot\n",
    "evaluator.calculate_top5_accuracy()       # Top-5 Accuracy\n",
    "evaluator.plot_roc_curve()              # ROC Curve\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added extra convolutinal layer\n",
    "CNNs work on the principle of learning higher level features proggressively from input iages through a number of convolutional, activation, and pooling layers, each layer extracts features from the pictures at a differenat level of complexity:\n",
    "The first layer looks out for basic patterns like edges, simple shapes, and textures\n",
    "The second layer combines these basic patters making larger more complex features\n",
    "The third layer observes more abstracts features whihc could be things like object parts\n",
    "Lastly the fourth layer which we added will combine the high level features so that it is able to create a meaningful representation of the objects in the picture.\n",
    "\n",
    "The reason behind me adding this extra layer is the following:\n",
    "1. avoid the model underfitting to the data, by adding an extra layer, the chance of the model not capturing the complexity of the data is lower, therefore allowing the model to have a notion of more intricate relationship between features.\n",
    "2. the extra convolutional layer, can become awware more complex features which may be overlooked by a model with only 3 layers, this in turn increases representational power.\n",
    "3. A dataset like CIFAR-10 includes images with vairous different scales and objects of different sizes. A deeper more thorough network will perform better at capturing these occasions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used a learning rate shceduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning scheduler makes use of specific strategies to adjust the learning rate which is used during training dynamically. The strategy I went for was ReduceLROnPlateau, which allows the model to reduce the learning rate by a factor whenever the validation loss plateaus.\n",
    "\n",
    "My motivation behind this choice lies in the following thoughts:\n",
    "1. giving automatic control to change the learning rate whenever the validation loss stops improving, removing the need for us to manually make those adjustments, leading to a simpler training process\n",
    "2. As a high learning rate can at certain points cause the halting of improvements, having it change as required allows us to maintain our models abiltiy to improve, due to removing the risk of oscillating between optimal weights\n",
    "3. By decreasing the size of the steps the model takes when training, allows the model to settle at a more optimal solution rather then if the learning rate was constant while still maintainig efficency and ensuring overshooting doesn't occur once the minima is reached \n",
    "4. Higher learning rates at the start help models converge faster, while lowering the learning rate later on leads us to the previous point.\n",
    "\n",
    "I settled on ReduceLROnPlateau as it monitors the validation loss directly, only adjusting learning rates when no improvements are observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved CNN Model I will only be adding comments which wouldn't be a repeat of the base class ones\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.convolution1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.convolution2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.convolution3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Added Fourth Convolutional Layer\n",
    "        self.convolution4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 256) # Same as original model\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5) # Same as original model\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.normaliseBatch1(self.convolution1(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch2(self.convolution2(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch3(self.convolution3(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch4(self.convolution4(x))))\n",
    "        x = x.view(-1, 256 * 2 * 2) # due to the fourth convolution layer the output channel size increases\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "model = ImprovedCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001) # Same as original model\n",
    "#schduler reduces learning rate of the optimiser when teh learning rate stops improving, reduces lr by a factor of 0.5, \n",
    "#patience is how long to wait after last improvement to reduce lr again, so if validation loss does not improve after 3 epochs, lr is reduced by a factor of 0.5\n",
    "#verbose shows a message when lr is reduced \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Create an instance of ModelEvaluator after training\n",
    "evaluator = ModelEvaluator(model, optimiser, criterion, device, load_train, load_validation, load_test)\n",
    "\n",
    "# Training Loop\n",
    "epochs =20  # Same as original model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in load_train:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / len(load_train)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Validation Check\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in load_validation:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    average_validation_loss = validation_loss / len(load_test)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {average_validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    scheduler.step(average_validation_loss)\n",
    "    \n",
    "# Final Test Accuracy\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in load_validation:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate model on the test set\n",
    "evaluator.evaluate_epoch(epochs - 1)\n",
    "\n",
    "# Call all evaluation methods\n",
    "evaluator.plot_accuracy()               # Plot Accuracy\n",
    "evaluator.plot_validation_loss()        # Plot Validation Loss\n",
    "evaluator.plot_training_loss()          # Plot Training Loss\n",
    "evaluator.plot_confusion_matrix()       # Confusion Matrix\n",
    "evaluator.print_classification_report() # Classification Report\n",
    "evaluator.plot_precision_recall_curve()# Precision-Recall Curve\n",
    "evaluator.plot_learning_rate()          # Learning Rate Plot\n",
    "evaluator.calculate_top5_accuracy()       # Top-5 Accuracy\n",
    "evaluator.plot_roc_curve()              # ROC Curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimised hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated CNN Model\n",
    "class OptimisedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimisedCNN, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.convolution1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.convolution2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.convolution3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Added Fourth Convolutional Layer\n",
    "        self.convolution4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.normaliseBatch4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "     \n",
    "        self.dropout = nn.Dropout(0.5) # 50% of neurons will be dropeed out during each forward pass in training\n",
    "        #or use this one\n",
    "        #self.dropout = nn.Dropout(0.3) # 30% of neurons will be dropeed out during each forward pass in training\n",
    "\n",
    "        self.relu = nn.ReLU() #defines the activation functiuon ReLU to introduce non linearity, sets negative values to zero and keeps positive values. \n",
    "        #or use this one\n",
    "        #self.relu = nn.LeakyReLU(negative_slope=0.1) #leaky ReLU unlike ReLU allows a slight negative slope for negative inputs, preventing neurons from dying keeping them a little active.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.normaliseBatch1(self.convolution1(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch2(self.convolution2(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch3(self.convolution3(x))))\n",
    "        x = self.pool(self.relu(self.normaliseBatch4(self.convolution4(x))))\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Model, Loss, Optimizer, and Scheduler\n",
    "model = OptimisedCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimiser = optim.Adam(model.parameters(), lr=0.001) # auto adjusts learning rate for each parameter while training\n",
    "#or use this one... could further change the learning rate lr to 0.005\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) #traditional optimiser, adjusts parameters uniformly based on gradients, uses momentum and weight decay for stable and efficient updates\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=3, verbose=True) #reduces learning rate when validation loss stops improving, does this dynamically to prevent stagnation\n",
    "#or this one\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=0.01, steps_per_epoch=len(load_train), epochs= 10) #increases and decreases learning rate in cycles over the entire training, It has a triangular learning rate policy, lr goes up fast in the first phase, then peaks at max_lr and starts decaying to values near zero.\n",
    "\n",
    "# Create an instance of ModelEvaluator after training\n",
    "evaluator = ModelEvaluator(model, optimiser, criterion, device, load_train, load_validation, load_test)\n",
    "\n",
    "\n",
    "# Increase Training Epochs\n",
    "epochs = 10\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in load_train:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / len(load_train)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Validation Check\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in load_validation:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    average_validation_loss = validation_loss / len(load_test)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {average_validation_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    scheduler.step(average_validation_loss)\n",
    "\n",
    "# Final Test Accuracy\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in load_test:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate model on the test set\n",
    "evaluator.evaluate_epoch(epochs - 1)\n",
    "\n",
    "# Call all evaluation methods\n",
    "evaluator.plot_accuracy()               # Plot Accuracy\n",
    "evaluator.plot_validation_loss()        # Plot Validation Loss\n",
    "evaluator.plot_training_loss()          # Plot Training Loss\n",
    "evaluator.plot_confusion_matrix()       # Confusion Matrix\n",
    "evaluator.print_classification_report() # Classification Report\n",
    "evaluator.plot_precision_recall_curve()# Precision-Recall Curve\n",
    "evaluator.plot_learning_rate()          # Learning Rate Plot\n",
    "evaluator.calculate_top5_accuracy()       # Top-5 Accuracy\n",
    "evaluator.plot_roc_curve()              # ROC Curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

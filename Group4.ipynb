{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Mathematics and Programming in Artificail intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Why CIFAR-10](#why-cifar-10)\n",
    "2. Task 1: NumPy Neural Network Implementation\n",
    "   - [Activation Functions](#Activation-Functions)\n",
    "   - [Softmax Layer](#Softmax-Layer)\n",
    "   - [Dropout Implementation](#Dropout-Implementation)\n",
    "   - [Neural Network Class](#neural-network-class)\n",
    "   - [Optimisers](#optimisers)\n",
    "   - [Network Evaluation and Results](#network-evaluation-and-results)\n",
    "3. Task 2: PyTorch Implementation\n",
    "   - [Dataset Preparation](#dataset-preparation)\n",
    "   - [Model Description and Implementation](#model-description-and-implementation)\n",
    "   - [Improvements](#improvements)\n",
    "   - [Hyperparameter Optimisation](#hyperparameter-optimisation)\n",
    "   - [Results and Discussion](#results-and-discussion)\n",
    "4. [Conclusion and Reflection](#conclusion-and-reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose CIFAR-10 dataset due to its complexity and suitability in evaluating multi-layer neural networks. It holds 60,000 32x32 colour images spanning 10 diverse classes, crescendoing in a challenging classification task surpassing the likes of datasets like MNIST, which only holds grayscale digits. CIFAR-10 includes RGB images, demanding models to learn from more detailed and complex data, mirroring real world applications, where data is diverse and high dimensional. The aforementioned complexity allows rigorous testing of network architectures, activation functions, and techniques to optimise the model. Lastly, it has well documented benchmarks, and  widespread use in academic research making it a top candidate to showcase advanced implementations, setting the ground work for meaningful comparisons and evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: NumPy Neural Network Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    @staticmethod\n",
    "    def sigmoidForward(x):\n",
    "        '''\n",
    "        this function does the forward pass of the sigmoid function\n",
    "        it takes an input array i just called x \n",
    "        and it returns a tuple with the result (out) of the sigmoid function\n",
    "        as well as cache which we use in the backward pass, its just the same as the out \n",
    "        '''\n",
    "        out = 1/ (1+ np.exp(-x)) #the sigmoid function\n",
    "        cache = out \n",
    "        return out, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoidBackward(dout, cache):\n",
    "        '''\n",
    "        does the backward pass of the simoid function\n",
    "        i used d to show that its the derivative \n",
    "        so dx is the gradient of the loss with resepct to x (the input array)\n",
    "        dout is the upstream gradient\n",
    "        sig is just the sigmoid function hence why it equals cache\n",
    "        '''\n",
    "        sig = cache\n",
    "        dx = dout * sig * (1 - sig) #the derivative of the sigmoid function multiplied by the upstream gradient to get the proper flow of gradients\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    @staticmethod   \n",
    "    def reluForward(x):\n",
    "        '''\n",
    "        does the forward pass of the ReLU function\n",
    "        x is the input arry \n",
    "        and then it outputs a tuple with the result of the  ReLU forward pass as well a cache used for backward pass\n",
    "        cache this time is the input array \n",
    "        '''\n",
    "        out = np.maximum(0,x)\n",
    "        cache = x \n",
    "        return out, cache\n",
    "    @staticmethod\n",
    "    def reluBackward(dout, cache):\n",
    "        '''\n",
    "        backward pass of the ReLU function\n",
    "        x is just passing on the inpui arrat from forward pass using cache as the temporary store \n",
    "        dx is the gradient of the loss in respect to the input (being the array x)\n",
    "        dout is the upstream gradient \n",
    "        '''\n",
    "        x = cache \n",
    "        dx = dout * (x > 0) #derivative is 1 when x >0 otherwise it is 0\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        # Prepare to store the output of the softmax function\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, logits):\n",
    "        \"\"\"\n",
    "        Perform the forward pass to calculate softmax probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        logits (np.array): Scores from the previous layer, shaped (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "        np.array: Probabilities for each class, same shape as input.\n",
    "        \"\"\"\n",
    "        # Subtract the max to keep numbers stable\n",
    "        z_max = np.max(logits, axis=1, keepdims=True)\n",
    "        shifted_logits = logits - z_max\n",
    "        exp_shifted = np.exp(shifted_logits)\n",
    "\n",
    "        # Divide by sum of exponents to get probabilities\n",
    "        sum_exp = np.sum(exp_shifted, axis=1, keepdims=True)\n",
    "        self.output = exp_shifted / sum_exp\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, true_labels):\n",
    "        \"\"\"\n",
    "        Perform the backward pass to calculate gradient of the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        true_labels (np.array): One-hot encoded true class labels.\n",
    "\n",
    "        Returns:\n",
    "        np.array: Gradient of the loss with respect to logits.\n",
    "        \"\"\"\n",
    "        # Get the number of samples to average the gradient\n",
    "        num_samples = true_labels.shape[0]\n",
    "\n",
    "        # Calculate the gradient for softmax combined with cross-entropy\n",
    "        gradient = (self.output - true_labels) / num_samples\n",
    "        return gradient\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage of SoftmaxLayer\n",
    "\n",
    "    # Define example logits, representing class scores\n",
    "    logits_example = np.array([[2.0, 1.0, 0.1],\n",
    "                               [1.0, 2.0, 0.1]])\n",
    "\n",
    "    # Define the true labels in one-hot encoding\n",
    "    true_labels_example = np.array([[1, 0, 0],\n",
    "                                    [0, 1, 0]])\n",
    "\n",
    "    # Instantiate the softmax layer\n",
    "    softmax_layer = SoftmaxLayer()\n",
    "\n",
    "    # Forward pass to get probability distributions\n",
    "    softmax_output = softmax_layer.forward(logits_example)\n",
    "    print(\"Softmax Probabilities:\\n\", softmax_output)\n",
    "\n",
    "    # Backward pass to compute gradient\n",
    "    loss_gradient = softmax_layer.backward(true_labels_example)\n",
    "    print(\"Gradient of Loss w.r.t Logits:\\n\", loss_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Author: Abdelrahmane Bekhli\n",
    "    Date: 2024-11-18\n",
    "    Description: This class performs dropouts.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropoutRate, seed=None):\n",
    "        \"\"\" \n",
    "        Initialize the Dropout layer. \n",
    "        Args: \n",
    "            dropoutRate (float): The probability of dropping out a unit. \n",
    "            seed (int, optional): Random seed for reproducibility. \n",
    "        \"\"\"\n",
    "        if not (0 <= dropoutRate < 1):\n",
    "            raise ValueError(\"Dropout rate must be between 0 and 1\")\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for dropout.\n",
    "        Args:\n",
    "            x (numpy array): The input to the dropout layer.\n",
    "        Returns:\n",
    "            numpy array: Output after applying dropout.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropoutRate\n",
    "            return x * self.mask / (1 - self.dropoutRate)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass for dropout.\n",
    "        Args:\n",
    "            dout (numpy array): The gradient from the next layer.\n",
    "        Returns:\n",
    "            numpy array: Gradient after applying dropout mask.\n",
    "        \"\"\"\n",
    "        return dout * self.mask / (1 - self.dropoutRate)\n",
    "    \n",
    "    def setMode(self, mode):\n",
    "        \"\"\"\n",
    "        Set the mode for the network: 'train' or 'test'\n",
    "        Args:\n",
    "            mode (str): Either 'train' or 'test'.\n",
    "        \"\"\"\n",
    "        if mode == 'train':\n",
    "            self.training = True\n",
    "        elif mode == 'test':\n",
    "            self.training = False\n",
    "        else:\n",
    "            raise ValueError(\"Mode can only be 'train' or 'test'\")\n",
    "        self.mask = None # reset mask when changing modes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
